# BeyondChats Article Scraper & Enhancer

A full-stack application that scrapes articles from BeyondChats blog, enhances them using AI (Groq LLM), and displays both original and enhanced versions in a responsive React frontend.

## üåê Live Demo

**Live Application:** [https://beyondchats-ocbd.onrender.com](https://beyondchats-ocbd.onrender.com)

**API Endpoint:** [https://beyondchats-ocbd.onrender.com/api/articles](https://beyondchats-ocbd.onrender.com/api/articles)

---

## üìã Project Overview

This project fulfills the BeyondChats Full Stack Developer Intern assignment with three phases:

| Phase | Description | Status |
|-------|-------------|--------|
| **Phase 1** | Scrape articles from BeyondChats blog, store in database, create CRUD APIs | ‚úÖ Complete |
| **Phase 2** | NodeJS script to search Google, scrape references, enhance with LLM, publish | ‚úÖ Complete |
| **Phase 3** | React frontend to display original and enhanced articles | ‚úÖ Complete |

---

## üèóÔ∏è Architecture Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                           BeyondChats Article Scraper                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend   ‚îÇ     ‚îÇ   Backend    ‚îÇ     ‚îÇ   Scraper    ‚îÇ     ‚îÇ  External    ‚îÇ
‚îÇ   (React)    ‚îÇ     ‚îÇ  (Express)   ‚îÇ     ‚îÇ  (Node.js)   ‚îÇ     ‚îÇ  Services    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                    ‚îÇ                    ‚îÇ                    ‚îÇ
       ‚îÇ  HTTP/REST         ‚îÇ                    ‚îÇ                    ‚îÇ
       ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ                    ‚îÇ                    ‚îÇ
       ‚îÇ                    ‚îÇ                    ‚îÇ                    ‚îÇ
       ‚îÇ                    ‚îÇ  SQLite            ‚îÇ                    ‚îÇ
       ‚îÇ                    ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ                    ‚îÇ
       ‚îÇ                    ‚îÇ         ‚îÇ          ‚îÇ                    ‚îÇ
       ‚îÇ                    ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ                    ‚îÇ
       ‚îÇ                    ‚îÇ    ‚îÇDatabase ‚îÇ     ‚îÇ                    ‚îÇ
       ‚îÇ                    ‚îÇ    ‚îÇ(SQLite) ‚îÇ     ‚îÇ                    ‚îÇ
       ‚îÇ                    ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ                    ‚îÇ
       ‚îÇ                    ‚îÇ                    ‚îÇ                    ‚îÇ
       ‚îÇ                    ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ Store Articles     ‚îÇ
       ‚îÇ                    ‚îÇ                    ‚îÇ                    ‚îÇ
       ‚îÇ                    ‚îÇ                    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
       ‚îÇ                    ‚îÇ                    ‚îÇ  Scrape BeyondChats‚îÇ
       ‚îÇ                    ‚îÇ                    ‚îÇ                    ‚îÇ
       ‚îÇ                    ‚îÇ                    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
       ‚îÇ                    ‚îÇ                    ‚îÇ  Google Search     ‚îÇ
       ‚îÇ                    ‚îÇ                    ‚îÇ                    ‚îÇ
       ‚îÇ                    ‚îÇ                    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
       ‚îÇ                    ‚îÇ                    ‚îÇ  Scrape References ‚îÇ
       ‚îÇ                    ‚îÇ                    ‚îÇ                    ‚îÇ
       ‚îÇ                    ‚îÇ                    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ
       ‚îÇ                    ‚îÇ                    ‚îÇ  Groq LLM API      ‚îÇ
       ‚îÇ                    ‚îÇ                    ‚îÇ                    ‚îÇ
```

---

## üìä Data Flow Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                              DATA FLOW                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1. SCRAPING PHASE
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ BeyondChats ‚îÇ ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ   Scraper   ‚îÇ ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ  Backend    ‚îÇ
   ‚îÇ    Blog     ‚îÇ      ‚îÇ  (Puppeteer)‚îÇ      ‚îÇ    API      ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                    ‚îÇ
                                                    ‚ñº
                                             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                             ‚îÇ   SQLite    ‚îÇ
                                             ‚îÇ  Database   ‚îÇ
                                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

2. ENHANCEMENT PHASE
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ   Google    ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ ‚îÇ   Scraper   ‚îÇ ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ  Reference  ‚îÇ
   ‚îÇ   Search    ‚îÇ      ‚îÇ             ‚îÇ      ‚îÇ  Websites   ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
                               ‚ñº
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇ  Groq LLM   ‚îÇ
                        ‚îÇ    API      ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
                               ‚ñº
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇ  Enhanced   ‚îÇ
                        ‚îÇ  Articles   ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

3. DISPLAY PHASE
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ   React     ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ ‚îÇ  Backend    ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ ‚îÇ   SQLite    ‚îÇ
   ‚îÇ  Frontend   ‚îÇ      ‚îÇ    API      ‚îÇ      ‚îÇ  Database   ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è Tech Stack

| Component | Technology |
|-----------|------------|
| **Frontend** | React 18, TypeScript, Vite, Tailwind CSS, Axios |
| **Backend** | Node.js, Express, TypeScript, SQLite, better-sqlite3 |
| **Scraper** | Node.js, TypeScript, Puppeteer, Groq SDK |
| **LLM** | Groq API (Llama 3.1 8B) |
| **Testing** | Jest, fast-check (property-based testing) |

---

## üìÅ Project Structure

```
beyondchats-article-scraper/
‚îú‚îÄ‚îÄ backend/                 # Express.js REST API
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ controllers/     # Request handlers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/          # Database models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes/          # API routes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ middleware/      # Error handling
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ db/              # Database setup & migrations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils/           # Logger utility
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îÇ
‚îú‚îÄ‚îÄ frontend/                # React frontend
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/      # React components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/             # API client
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ types/           # TypeScript types
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ App.tsx          # Main app component
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îÇ
‚îú‚îÄ‚îÄ scraper/                 # Article scraper & enhancer
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/        # Scraping & LLM services
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config/          # Configuration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ types/           # TypeScript types
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.ts         # Main entry point
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îÇ
‚îî‚îÄ‚îÄ README.md
```

---

## üöÄ Local Setup Instructions

### Prerequisites

- Node.js 18+ 
- npm or yarn
- Chrome/Chromium (for Puppeteer)

### 1. Clone the Repository

```bash
git clone https://github.com/anjali04853/BeyondChats.git
cd BeyondChats
```

### 2. Install Dependencies

```bash
# Install root dependencies
npm install

# Install backend dependencies
cd backend && npm install && cd ..

# Install frontend dependencies
cd frontend && npm install && cd ..

# Install scraper dependencies
cd scraper && npm install && cd ..
```

### 3. Configure Environment Variables

**Backend** (`backend/.env`):
```env
PORT=3001
NODE_ENV=development
```

**Scraper** (`scraper/.env`):
```env
# API Configuration
API_BASE_URL=http://localhost:3001/api

# Puppeteer Configuration
PUPPETEER_HEADLESS=true
PUPPETEER_TIMEOUT=30000

# Groq LLM Configuration (Free API)
GROQ_API_KEY=your_groq_api_key_here
LLM_MODEL=llama-3.1-8b-instant

# Logging
LOG_LEVEL=info
```

> üí° Get a free Groq API key at [console.groq.com](https://console.groq.com)

### 4. Start the Backend

```bash
cd backend
npm run dev
```

The API will be available at `http://localhost:3001`

### 5. Start the Frontend

```bash
cd frontend
npm run dev
```

The frontend will be available at `http://localhost:3000`

### 6. Run the Scraper

```bash
cd scraper
npm run dev
```

This will:
1. Scrape 5 articles from BeyondChats blog
2. Store them in the database
3. Search Google for reference articles
4. Enhance articles using Groq LLM
5. Store enhanced versions

---

## üì° API Endpoints

### Articles

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/articles` | Get all articles (paginated) |
| `GET` | `/api/articles/:id` | Get article by ID |
| `POST` | `/api/articles` | Create new article |
| `PUT` | `/api/articles/:id` | Update article |
| `DELETE` | `/api/articles/:id` | Delete article |
| `GET` | `/api/articles/:id/enhanced` | Get enhanced version |

### Enhanced Articles

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/enhanced-articles` | Get all enhanced articles |
| `GET` | `/api/enhanced-articles/:id` | Get enhanced article by ID |
| `POST` | `/api/enhanced-articles` | Create enhanced article |

### Example Response

```json
{
  "articles": [
    {
      "id": 1,
      "title": "AI in Healthcare: Hype or Reality?",
      "content": "...",
      "author": "Simran Jain",
      "publication_date": "2025-03-25",
      "source_url": "https://beyondchats.com/blogs/...",
      "created_at": "2025-12-29T10:00:00.000Z"
    }
  ],
  "pagination": {
    "page": 1,
    "limit": 10,
    "total": 5,
    "totalPages": 1
  }
}
```

---

## ‚ú® Features

### Phase 1: Web Scraping & CRUD APIs
- ‚úÖ Scrapes 5 oldest articles from BeyondChats blog
- ‚úÖ Extracts title, content, author, publication date
- ‚úÖ Stores in SQLite database
- ‚úÖ Full CRUD REST API

### Phase 2: AI Enhancement
- ‚úÖ Searches article titles on Google
- ‚úÖ Fetches first 2 blog/article links (excludes beyondchats.com)
- ‚úÖ Scrapes reference article content
- ‚úÖ Enhances articles using Groq LLM (Llama 3.1)
- ‚úÖ Adds citations to reference articles
- ‚úÖ Publishes enhanced articles via API

### Phase 3: React Frontend
- ‚úÖ Responsive, professional UI with Tailwind CSS
- ‚úÖ Article list with pagination
- ‚úÖ Article detail view
- ‚úÖ Toggle between Original and Enhanced versions
- ‚úÖ Side-by-side comparison view
- ‚úÖ Mobile-first design

---

## üß™ Running Tests

```bash
# Backend tests
cd backend && npm test

# Scraper tests
cd scraper && npm test

# Frontend tests
cd frontend && npm test
```

---

## üì∏ Screenshots

### Article List View
Browse all scraped articles in a clean, responsive grid layout with pagination. Each article card displays the title, excerpt, author, and publication date.

![Article List](https://raw.githubusercontent.com/anjali04853/BeyondChats/main/screenshots/article-list.png)

### Article Detail with Original/Enhanced Toggle
View individual articles with the ability to toggle between the original BeyondChats version and the AI-enhanced version using Groq LLM. Features include:
- Original article content from BeyondChats
- Enhanced version with improved formatting and content
- Author and publication date information
- Link to original source
- Side-by-side comparison capability

![Article Detail](https://raw.githubusercontent.com/anjali04853/BeyondChats/main/screenshots/article-detail.png)

---

## üîß Configuration Options

### Scraper Configuration

| Variable | Default | Description |
|----------|---------|-------------|
| `API_BASE_URL` | `http://localhost:3001/api` | Backend API URL |
| `PUPPETEER_HEADLESS` | `true` | Run browser in headless mode |
| `PUPPETEER_TIMEOUT` | `30000` | Page load timeout (ms) |
| `GROQ_API_KEY` | - | Groq API key for LLM |
| `LLM_MODEL` | `llama-3.1-8b-instant` | LLM model to use |

---

## üìù Development Notes

- The scraper uses Puppeteer for web scraping to handle JavaScript-rendered content
- Google search results are filtered to exclude beyondchats.com and return only blog/article links
- Groq's free tier provides fast inference with Llama models
- SQLite is used for simplicity; can be replaced with PostgreSQL for production
- Property-based tests ensure correctness across many input variations

---

## üë§ Author

**Anjali Verma**

---

## üìÑ License

This project is created for the BeyondChats Full Stack Developer Intern assignment.

**GitHub Repository:** [https://github.com/anjali04853/BeyondChats.git](https://github.com/anjali04853/BeyondChats.git)
